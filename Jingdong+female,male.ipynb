{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dfaca1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e490ffe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 500 search results to C:\\Users\\枫\\Desktop\\女性服装.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def crawl_jd_search_results(keyword, max_results):\n",
    "    url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page=1'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    page = 1\n",
    "    while len(results) < max_results:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        products = soup.select('.gl-item')\n",
    "\n",
    "        for product in products:\n",
    "            name = product.select_one('.p-name em').text.strip()\n",
    "            price = product.select_one('.p-price').text.strip()\n",
    "            results.append({'Name': name, 'Price': price})\n",
    "\n",
    "            if len(results) >= max_results:\n",
    "                break\n",
    "\n",
    "        page += 1\n",
    "        url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page={page}'\n",
    "\n",
    "    return results[:max_results]\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    keys = data[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "def main():\n",
    "    keyword = '女性服装'\n",
    "    max_results = 500\n",
    "    filename = os.path.join(os.path.expanduser(\"~\"), 'Desktop', '女性服装.csv')\n",
    "\n",
    "    search_results = crawl_jd_search_results(keyword, max_results)\n",
    "    save_to_csv(search_results, filename)\n",
    "    print(f'Saved {len(search_results)} search results to {filename}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f59ff387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 500 search results to C:\\Users\\枫\\Desktop\\男性服装.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def crawl_jd_search_results(keyword, max_results):\n",
    "    url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page=1'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    page = 1\n",
    "    while len(results) < max_results:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        products = soup.select('.gl-item')\n",
    "\n",
    "        for product in products:\n",
    "            name = product.select_one('.p-name em').text.strip()\n",
    "            price = product.select_one('.p-price').text.strip()\n",
    "            results.append({'Name': name, 'Price': price})\n",
    "\n",
    "            if len(results) >= max_results:\n",
    "                break\n",
    "\n",
    "        page += 1\n",
    "        url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page={page}'\n",
    "\n",
    "    return results[:max_results]\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    keys = data[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "def main():\n",
    "    keyword = '男性服装'\n",
    "    max_results = 500\n",
    "    filename = os.path.join(os.path.expanduser(\"~\"), 'Desktop', '男性服装.csv')\n",
    "\n",
    "    search_results = crawl_jd_search_results(keyword, max_results)\n",
    "    save_to_csv(search_results, filename)\n",
    "    print(f'Saved {len(search_results)} search results to {filename}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "387222be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 500 search results to C:\\Users\\枫\\Desktop\\女性运动用品.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def crawl_jd_search_results(keyword, max_results):\n",
    "    url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page=1'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    page = 1\n",
    "    while len(results) < max_results:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        products = soup.select('.gl-item')\n",
    "\n",
    "        for product in products:\n",
    "            name = product.select_one('.p-name em').text.strip()\n",
    "            price = product.select_one('.p-price').text.strip()\n",
    "            results.append({'Name': name, 'Price': price})\n",
    "\n",
    "            if len(results) >= max_results:\n",
    "                break\n",
    "\n",
    "        page += 1\n",
    "        url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page={page}'\n",
    "\n",
    "    return results[:max_results]\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    keys = data[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "def main():\n",
    "    keyword = '女性运动用品'\n",
    "    max_results = 500\n",
    "    filename = os.path.join(os.path.expanduser(\"~\"), 'Desktop', '女性运动用品.csv')\n",
    "\n",
    "    search_results = crawl_jd_search_results(keyword, max_results)\n",
    "    save_to_csv(search_results, filename)\n",
    "    print(f'Saved {len(search_results)} search results to {filename}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92ac7c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 500 search results to C:\\Users\\枫\\Desktop\\男性运动用品.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def crawl_jd_search_results(keyword, max_results):\n",
    "    url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page=1'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    page = 1\n",
    "    while len(results) < max_results:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        products = soup.select('.gl-item')\n",
    "\n",
    "        for product in products:\n",
    "            name = product.select_one('.p-name em').text.strip()\n",
    "            price = product.select_one('.p-price').text.strip()\n",
    "            results.append({'Name': name, 'Price': price})\n",
    "\n",
    "            if len(results) >= max_results:\n",
    "                break\n",
    "\n",
    "        page += 1\n",
    "        url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page={page}'\n",
    "\n",
    "    return results[:max_results]\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    keys = data[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "def main():\n",
    "    keyword = '男性运动用品'\n",
    "    max_results = 500\n",
    "    filename = os.path.join(os.path.expanduser(\"~\"), 'Desktop', '男性运动用品.csv')\n",
    "\n",
    "    search_results = crawl_jd_search_results(keyword, max_results)\n",
    "    save_to_csv(search_results, filename)\n",
    "    print(f'Saved {len(search_results)} search results to {filename}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f082c200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 500 search results to C:\\Users\\枫\\Desktop\\女性个人护理.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def crawl_jd_search_results(keyword, max_results):\n",
    "    url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page=1'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    page = 1\n",
    "    while len(results) < max_results:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        products = soup.select('.gl-item')\n",
    "\n",
    "        for product in products:\n",
    "            name = product.select_one('.p-name em').text.strip()\n",
    "            price = product.select_one('.p-price').text.strip()\n",
    "            results.append({'Name': name, 'Price': price})\n",
    "\n",
    "            if len(results) >= max_results:\n",
    "                break\n",
    "\n",
    "        page += 1\n",
    "        url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page={page}'\n",
    "\n",
    "    return results[:max_results]\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    keys = data[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "def main():\n",
    "    keyword = '女性个人护理'\n",
    "    max_results = 500\n",
    "    filename = os.path.join(os.path.expanduser(\"~\"), 'Desktop', '女性个人护理.csv')\n",
    "\n",
    "    search_results = crawl_jd_search_results(keyword, max_results)\n",
    "    save_to_csv(search_results, filename)\n",
    "    print(f'Saved {len(search_results)} search results to {filename}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00996b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 500 search results to C:\\Users\\枫\\Desktop\\男性个人护理.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def crawl_jd_search_results(keyword, max_results):\n",
    "    url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page=1'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    page = 1\n",
    "    while len(results) < max_results:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        products = soup.select('.gl-item')\n",
    "\n",
    "        for product in products:\n",
    "            name = product.select_one('.p-name em').text.strip()\n",
    "            price = product.select_one('.p-price').text.strip()\n",
    "            results.append({'Name': name, 'Price': price})\n",
    "\n",
    "            if len(results) >= max_results:\n",
    "                break\n",
    "\n",
    "        page += 1\n",
    "        url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page={page}'\n",
    "\n",
    "    return results[:max_results]\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    keys = data[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "def main():\n",
    "    keyword = '男性个人护理'\n",
    "    max_results = 500\n",
    "    filename = os.path.join(os.path.expanduser(\"~\"), 'Desktop', '男性个人护理.csv')\n",
    "\n",
    "    search_results = crawl_jd_search_results(keyword, max_results)\n",
    "    save_to_csv(search_results, filename)\n",
    "    print(f'Saved {len(search_results)} search results to {filename}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36db66dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 500 search results to C:\\Users\\枫\\Desktop\\男性电子产品.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def crawl_jd_search_results(keyword, max_results):\n",
    "    url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page=1'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    page = 1\n",
    "    while len(results) < max_results:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        products = soup.select('.gl-item')\n",
    "\n",
    "        for product in products:\n",
    "            name = product.select_one('.p-name em').text.strip()\n",
    "            price = product.select_one('.p-price').text.strip()\n",
    "            results.append({'Name': name, 'Price': price})\n",
    "\n",
    "            if len(results) >= max_results:\n",
    "                break\n",
    "\n",
    "        page += 1\n",
    "        url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page={page}'\n",
    "\n",
    "    return results[:max_results]\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    keys = data[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "def main():\n",
    "    keyword = '男性电子产品'\n",
    "    max_results = 500\n",
    "    filename = os.path.join(os.path.expanduser(\"~\"), 'Desktop', '男性电子产品.csv')\n",
    "\n",
    "    search_results = crawl_jd_search_results(keyword, max_results)\n",
    "    save_to_csv(search_results, filename)\n",
    "    print(f'Saved {len(search_results)} search results to {filename}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82c50828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 500 search results to C:\\Users\\枫\\Desktop\\女性电子产品.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def crawl_jd_search_results(keyword, max_results):\n",
    "    url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page=1'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    page = 1\n",
    "    while len(results) < max_results:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        products = soup.select('.gl-item')\n",
    "\n",
    "        for product in products:\n",
    "            name = product.select_one('.p-name em').text.strip()\n",
    "            price = product.select_one('.p-price').text.strip()\n",
    "            results.append({'Name': name, 'Price': price})\n",
    "\n",
    "            if len(results) >= max_results:\n",
    "                break\n",
    "\n",
    "        page += 1\n",
    "        url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page={page}'\n",
    "\n",
    "    return results[:max_results]\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    keys = data[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "def main():\n",
    "    keyword = '女性电子产品'\n",
    "    max_results = 500\n",
    "    filename = os.path.join(os.path.expanduser(\"~\"), 'Desktop', '女性电子产品.csv')\n",
    "\n",
    "    search_results = crawl_jd_search_results(keyword, max_results)\n",
    "    save_to_csv(search_results, filename)\n",
    "    print(f'Saved {len(search_results)} search results to {filename}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfee8ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 500 search results to C:\\Users\\枫\\Desktop\\女鞋.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def crawl_jd_search_results(keyword, max_results):\n",
    "    url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page=1'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    page = 1\n",
    "    while len(results) < max_results:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        products = soup.select('.gl-item')\n",
    "\n",
    "        for product in products:\n",
    "            name = product.select_one('.p-name em').text.strip()\n",
    "            price = product.select_one('.p-price').text.strip()\n",
    "            results.append({'Name': name, 'Price': price})\n",
    "\n",
    "            if len(results) >= max_results:\n",
    "                break\n",
    "\n",
    "        page += 1\n",
    "        url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page={page}'\n",
    "\n",
    "    return results[:max_results]\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    keys = data[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "def main():\n",
    "    keyword = '女鞋'\n",
    "    max_results = 500\n",
    "    filename = os.path.join(os.path.expanduser(\"~\"), 'Desktop', '女鞋.csv')\n",
    "\n",
    "    search_results = crawl_jd_search_results(keyword, max_results)\n",
    "    save_to_csv(search_results, filename)\n",
    "    print(f'Saved {len(search_results)} search results to {filename}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31a38e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 500 search results to C:\\Users\\枫\\Desktop\\男鞋.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def crawl_jd_search_results(keyword, max_results):\n",
    "    url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page=1'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    page = 1\n",
    "    while len(results) < max_results:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        products = soup.select('.gl-item')\n",
    "\n",
    "        for product in products:\n",
    "            name = product.select_one('.p-name em').text.strip()\n",
    "            price = product.select_one('.p-price').text.strip()\n",
    "            results.append({'Name': name, 'Price': price})\n",
    "\n",
    "            if len(results) >= max_results:\n",
    "                break\n",
    "\n",
    "        page += 1\n",
    "        url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page={page}'\n",
    "\n",
    "    return results[:max_results]\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    keys = data[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "def main():\n",
    "    keyword = '男鞋'\n",
    "    max_results = 500\n",
    "    filename = os.path.join(os.path.expanduser(\"~\"), 'Desktop', '男鞋.csv')\n",
    "\n",
    "    search_results = crawl_jd_search_results(keyword, max_results)\n",
    "    save_to_csv(search_results, filename)\n",
    "    print(f'Saved {len(search_results)} search results to {filename}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09212d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 500 search results to C:\\Users\\枫\\Desktop\\女保健品.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def crawl_jd_search_results(keyword, max_results):\n",
    "    url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page=1'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    page = 1\n",
    "    while len(results) < max_results:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        products = soup.select('.gl-item')\n",
    "\n",
    "        for product in products:\n",
    "            name = product.select_one('.p-name em').text.strip()\n",
    "            price = product.select_one('.p-price').text.strip()\n",
    "            results.append({'Name': name, 'Price': price})\n",
    "\n",
    "            if len(results) >= max_results:\n",
    "                break\n",
    "\n",
    "        page += 1\n",
    "        url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page={page}'\n",
    "\n",
    "    return results[:max_results]\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    keys = data[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "def main():\n",
    "    keyword = '女保健品'\n",
    "    max_results = 500\n",
    "    filename = os.path.join(os.path.expanduser(\"~\"), 'Desktop', '女保健品.csv')\n",
    "\n",
    "    search_results = crawl_jd_search_results(keyword, max_results)\n",
    "    save_to_csv(search_results, filename)\n",
    "    print(f'Saved {len(search_results)} search results to {filename}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fdfb673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 500 search results to C:\\Users\\枫\\Desktop\\男保健品.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def crawl_jd_search_results(keyword, max_results):\n",
    "    url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page=1'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    page = 1\n",
    "    while len(results) < max_results:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        products = soup.select('.gl-item')\n",
    "\n",
    "        for product in products:\n",
    "            name = product.select_one('.p-name em').text.strip()\n",
    "            price = product.select_one('.p-price').text.strip()\n",
    "            results.append({'Name': name, 'Price': price})\n",
    "\n",
    "            if len(results) >= max_results:\n",
    "                break\n",
    "\n",
    "        page += 1\n",
    "        url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page={page}'\n",
    "\n",
    "    return results[:max_results]\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    keys = data[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "def main():\n",
    "    keyword = '男保健品'\n",
    "    max_results = 500\n",
    "    filename = os.path.join(os.path.expanduser(\"~\"), 'Desktop', '男保健品.csv')\n",
    "\n",
    "    search_results = crawl_jd_search_results(keyword, max_results)\n",
    "    save_to_csv(search_results, filename)\n",
    "    print(f'Saved {len(search_results)} search results to {filename}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "170af30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 500 search results to C:\\Users\\枫\\Desktop\\女性文具.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def crawl_jd_search_results(keyword, max_results):\n",
    "    url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page=1'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    page = 1\n",
    "    while len(results) < max_results:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        products = soup.select('.gl-item')\n",
    "\n",
    "        for product in products:\n",
    "            name = product.select_one('.p-name em').text.strip()\n",
    "            price = product.select_one('.p-price').text.strip()\n",
    "            results.append({'Name': name, 'Price': price})\n",
    "\n",
    "            if len(results) >= max_results:\n",
    "                break\n",
    "\n",
    "        page += 1\n",
    "        url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page={page}'\n",
    "\n",
    "    return results[:max_results]\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    keys = data[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "def main():\n",
    "    keyword = '女性文具'\n",
    "    max_results = 500\n",
    "    filename = os.path.join(os.path.expanduser(\"~\"), 'Desktop', '女性文具.csv')\n",
    "\n",
    "    search_results = crawl_jd_search_results(keyword, max_results)\n",
    "    save_to_csv(search_results, filename)\n",
    "    print(f'Saved {len(search_results)} search results to {filename}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26b33b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 500 search results to C:\\Users\\枫\\Desktop\\男性文具.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def crawl_jd_search_results(keyword, max_results):\n",
    "    url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page=1'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    page = 1\n",
    "    while len(results) < max_results:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        products = soup.select('.gl-item')\n",
    "\n",
    "        for product in products:\n",
    "            name = product.select_one('.p-name em').text.strip()\n",
    "            price = product.select_one('.p-price').text.strip()\n",
    "            results.append({'Name': name, 'Price': price})\n",
    "\n",
    "            if len(results) >= max_results:\n",
    "                break\n",
    "\n",
    "        page += 1\n",
    "        url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page={page}'\n",
    "\n",
    "    return results[:max_results]\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    keys = data[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "def main():\n",
    "    keyword = '男性文具'\n",
    "    max_results = 500\n",
    "    filename = os.path.join(os.path.expanduser(\"~\"), 'Desktop', '男性文具.csv')\n",
    "\n",
    "    search_results = crawl_jd_search_results(keyword, max_results)\n",
    "    save_to_csv(search_results, filename)\n",
    "    print(f'Saved {len(search_results)} search results to {filename}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fd9399c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 500 search results to C:\\Users\\枫\\Desktop\\男孩玩具.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def crawl_jd_search_results(keyword, max_results):\n",
    "    url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page=1'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    page = 1\n",
    "    while len(results) < max_results:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        products = soup.select('.gl-item')\n",
    "\n",
    "        for product in products:\n",
    "            name = product.select_one('.p-name em').text.strip()\n",
    "            price = product.select_one('.p-price').text.strip()\n",
    "            results.append({'Name': name, 'Price': price})\n",
    "\n",
    "            if len(results) >= max_results:\n",
    "                break\n",
    "\n",
    "        page += 1\n",
    "        url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page={page}'\n",
    "\n",
    "    return results[:max_results]\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    keys = data[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "def main():\n",
    "    keyword = '男孩玩具'\n",
    "    max_results = 500\n",
    "    filename = os.path.join(os.path.expanduser(\"~\"), 'Desktop', '男孩玩具.csv')\n",
    "\n",
    "    search_results = crawl_jd_search_results(keyword, max_results)\n",
    "    save_to_csv(search_results, filename)\n",
    "    print(f'Saved {len(search_results)} search results to {filename}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbcd803f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 500 search results to C:\\Users\\枫\\Desktop\\女孩玩具.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def crawl_jd_search_results(keyword, max_results):\n",
    "    url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page=1'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    page = 1\n",
    "    while len(results) < max_results:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        products = soup.select('.gl-item')\n",
    "\n",
    "        for product in products:\n",
    "            name = product.select_one('.p-name em').text.strip()\n",
    "            price = product.select_one('.p-price').text.strip()\n",
    "            results.append({'Name': name, 'Price': price})\n",
    "\n",
    "            if len(results) >= max_results:\n",
    "                break\n",
    "\n",
    "        page += 1\n",
    "        url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page={page}'\n",
    "\n",
    "    return results[:max_results]\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    keys = data[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "def main():\n",
    "    keyword = '女孩玩具'\n",
    "    max_results = 500\n",
    "    filename = os.path.join(os.path.expanduser(\"~\"), 'Desktop', '女孩玩具.csv')\n",
    "\n",
    "    search_results = crawl_jd_search_results(keyword, max_results)\n",
    "    save_to_csv(search_results, filename)\n",
    "    print(f'Saved {len(search_results)} search results to {filename}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0582f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def crawl_jd_search_results(keyword, max_results):\n",
    "    url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page=1'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    page = 1\n",
    "    while len(results) < max_results:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        products = soup.select('.gl-item')\n",
    "\n",
    "        for product in products:\n",
    "            name = product.select_one('.p-name em').text.strip()\n",
    "            price = product.select_one('.p-price').text.strip()\n",
    "            results.append({'Name': name, 'Price': price})\n",
    "\n",
    "            if len(results) >= max_results:\n",
    "                break\n",
    "\n",
    "        page += 1\n",
    "        url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page={page}'\n",
    "\n",
    "    return results[:max_results]\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    keys = data[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "def main():\n",
    "    keyword = '女孩玩具'\n",
    "    max_results = 500\n",
    "    filename = os.path.join(os.path.expanduser(\"~\"), 'Desktop', '女孩玩具.csv')\n",
    "\n",
    "    search_results = crawl_jd_search_results(keyword, max_results)\n",
    "    save_to_csv(search_results, filename)\n",
    "    print(f'Saved {len(search_results)} search results to {filename}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a511be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def crawl_jd_search_results(keyword, max_results):\n",
    "    # Construct the initial search URL with the provided keyword\n",
    "    url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page=1'\n",
    "\n",
    "    # Set headers to mimic a web browser\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    # Create an empty list to store the search results\n",
    "    results = []\n",
    "\n",
    "    # Initialize the page counter\n",
    "    page = 1\n",
    "\n",
    "    # Continue crawling until the desired number of results is reached\n",
    "    while len(results) < max_results:\n",
    "        # Send an HTTP GET request to fetch the page content\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract the product items from the search results\n",
    "        products = soup.select('.gl-item')\n",
    "\n",
    "        # Process each product item\n",
    "        for product in products:\n",
    "            # Extract the product name\n",
    "            name = product.select_one('.p-name em').text.strip()\n",
    "\n",
    "            # Extract the product price\n",
    "            price = product.select_one('.p-price').text.strip()\n",
    "\n",
    "            # Create a dictionary to store the product information\n",
    "            product_info = {'Name': name, 'Price': price}\n",
    "\n",
    "            # Add the product information to the results list\n",
    "            results.append(product_info)\n",
    "\n",
    "            # Break the loop if the desired number of results is reached\n",
    "            if len(results) >= max_results:\n",
    "                break\n",
    "\n",
    "        # Increment the page counter\n",
    "        page += 1\n",
    "\n",
    "        # Update the URL to fetch the next page of search results\n",
    "        url = f'https://search.jd.com/Search?keyword={keyword}&enc=utf-8&page={page}'\n",
    "\n",
    "    # Return the search results, limited to the desired number of results\n",
    "    return results[:max_results]\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    # Extract the keys from the first dictionary in the data list\n",
    "    keys = data[0].keys()\n",
    "\n",
    "    # Open the CSV file in write mode\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        # Create a CSV writer object\n",
    "        writer = csv.DictWriter(file, fieldnames=keys)\n",
    "\n",
    "        # Write the header row to the CSV file\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Write the data rows to the CSV file\n",
    "        writer.writerows(data)\n",
    "\n",
    "def main():\n",
    "    # Set the keyword for the search\n",
    "    keyword = '女性服装'\n",
    "\n",
    "    # Set the maximum number of search results to retrieve\n",
    "    max_results = 500\n",
    "\n",
    "    # Set the filename for saving the search results\n",
    "    filename = os.path.join(os.path.expanduser(\"~\"), 'Desktop', '女性服装.csv')\n",
    "\n",
    "    # Get the search results\n",
    "    search_results = crawl_jd_search_results(keyword, max_results)\n",
    "\n",
    "    # Save the search results to a CSV file\n",
    "    save_to_csv(search_results, filename)\n",
    "    print(f'Saved {len(search_results)} search results to {filename}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
